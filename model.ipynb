import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats.mstats import winsorize
from scipy import optimize
from sklearn import preprocessing
#from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

from datetime import datetime, date, time

# seconds between two datetime
def date_diff(dInit, dFin):
    return (datetime.fromisoformat(dFin) - datetime.fromisoformat(dInit)).total_seconds()

# days between two datetime. does not consider specific time
def date_diff_days(dInit, dFin):
    return ((datetime.fromisoformat(dFin)).date() - (datetime.fromisoformat(dInit)).date() ).days

# week day (Mon-Sun, 0-6)
def date_weekday(date):
    return datetime.fromisoformat(date).weekday()

# only the hours not minutes and seconds
def date_hour(date):
    return datetime.fromisoformat(date).hour

# how many times each hashed code is in the dataframe
def hash_to_occurrences(dataframe, column):
    dic = (dataframe[column].value_counts()).to_dict()
    return dataframe[column].map(dic, na_action="ignore")

# for each feature in dataframe, print num of unique elements, num of duplicates not NaN, num of NaN
def information(data):
    
    print("{0:30} {1:13} {2:10} {3:15}".format ("Column", "len(set(el))", "len(duplicates ", "len(nulls)"))
    print("{0:30} {1:13} {2:10} {3:15}".format ("      ", "            ", "      non-null)", "          "))
    print("{0:30} {1:13} {2:10} {3:15}".format ("------", "------------", "---------------", "----------"))

    for i, e in enumerate(data.columns):
        lst = data[e]
        el_set = set()
        el_duplicates_n = 0
        el_null_n = 0

        for x in lst:
            if pd.isnull(x):
                el_null_n += 1
            else:
                if x in el_set:
                    el_duplicates_n += 1
                else:
                    el_set.add(x)

        print("{0:30} {1:13} {2:10} {3:15}".format (e, len(el_set), el_duplicates_n, el_null_n))

#TODO change name with bigger_than_threshold        
def predict_with_threshold(p, thresh):
    if p >= thresh:
        return 1
    else:
        return 0
        
# given a model, X and a threshold, return Y 
def predicted_y_with_threshold(model, X_cross_val, thresh):
    predicted_y_prob = model.predict_proba(X_cross_val)
    predicted_y_prob_1 = predicted_y_prob[:,1]
    thresh_vec = [thresh]*len(predicted_y_prob_1)
    return list(map(predict_with_threshold, predicted_y_prob_1, thresh_vec))


# given c_th = (c, threshold) fit a LogisticRegression on train set 
#     then return corresponding F1 score for cross validation set
#TODO do not hardcode LogisticRegression, do not use global variables (X, Y)_(train, cross_val)
def f1_score_neg(c_th):
    model = LogisticRegression(class_weight="balanced", C=c_th[0])
    model.fit(X_train, Y_train)
    Y_pred = predicted_y_with_threshold(model, X_cross_val,c_th[1])
    f1_neg = -1 * f1_score(Y_cross_val, Y_pred)
    return f1_neg

# given c_th = (c, threshold) fit a LogisticRegression on train set 
#     then return a _modified_ F1 score for cross validation set
#TODO do not use global variables
def f1_score_neg_mod_th(c_th):
    model = LogisticRegression(class_weight="balanced", C=c_th[0])
    model.fit(X_train, Y_train)
    Y_pred = predicted_y_with_threshold(model, X_cross_val,c_th[1])
    Y_pred_series = pd.Series(list(Y_pred))
    
    f1_mod_neg = -1*f1_mod(Y_cross_val_series, Y_pred_series)
    return f1_mod_neg


#modified F1_score for cross validation set
def f1_mod(Y_cross_val_series, Y_pred_series):
    
    tp = ((Y_pred_series == 1) & (Y_cross_val_series == 1)).sum() 
    fp = ((Y_pred_series == 1) & (Y_cross_val_series == 0)).sum()
    fn = ((Y_pred_series == 0) & (Y_cross_val_series == 1)).sum() 
    
    precision = tp/(tp+fp)
    recall_mod = (tp/(tp+fn))**(4)

    f1_mod = 2*precision*recall_mod/(precision+recall_mod)
    return f1_mod
    

# Read excel file and convert to csv. csv is faster to read
# excel_file = pd.read_excel ("sample_dataset_data_scientist.xlsx")
# excel_file.to_csv("sample_dataset_data_scientist.csv",header=True)

dataset = pd.read_csv("sample_dataset.csv")

data = dataset.copy()

print(data.info())

information(data)

#find redundant columns

for idx1, e1 in enumerate(data.columns):
    for idx2, e2 in enumerate(data.columns):
        if idx2 <= idx1:
            continue
        else:
            if len(data[e1].unique()) != len(data[e2].unique()):
                continue
            else:
                relations = []
                for el_col1, el_col2 in zip(data[e1], data[e2]):
                    if [el_col1, el_col2] not in relations:
                        relations.append([el_col1, el_col2])
                        if len(relations) > len(data[e1].unique()):
                            break
                if len(relations) == len(data[e1].unique()) == len(data[e2].unique()):
                    print(e1, "and", e2, "are redundant. The relation is", relations)
                
            

#delete countrycode. Alphacode to be checked against GeoIpCountry
data.drop(columns=["CountryCode"], inplace = True)

print(data.shape)

# check that TxnInitTime is smaller than TxnCompleteTime
(data.TxnInitTime <= data.TxnCompleteTime).all()

# check that FirstTransactionDate is smaller or equal than TxnCompleteTime
(data.FirstTransactionDate <= data.TxnCompleteTime).all()

#Note: FirstTransactionDate is correctly recorded when transaction is completed
print((data.FirstTransactionDate <= data.TxnInitTime).all())
print(sum(data.TxnInitTime < data.FirstTransactionDate))
data[data.TxnInitTime < data.FirstTransactionDate]

# check that FirstTransactionDate is smaller or equal than 2022-02-21 (when data is downloaded)
(data.FirstTransactionDate <= "2022-02-21").all()

# set appropriate type
data = data.astype(
    {"Merchant_Id":"category", 
     "ChannelType":"category", 
     "UserAgent":"category", 
     "PaymentChannel":"category", 
     "ItemName":"category"}) 

# strings to numbers
data["Merchant_Id"] = data["Merchant_Id"].cat.codes
data["ChannelType"] = data["ChannelType"].cat.codes
data["UserAgent"] = data["UserAgent"].cat.codes
data["PaymentChannel"] = data["PaymentChannel"].cat.codes
data["ItemName"] = data["ItemName"].cat.codes
#Alpha2Code and GeoIpCountry need same codes. Done later

# Fix NaN and add features

# where there is Email_Id but not FirstEmailDate, substitute FirstEmailDate with FirstTransactionDate
data.loc[ (~pd.isna(data.Email_Id)) & pd.isna(data.FirstEmailDate), "FirstEmailDate"] = data.loc[ (~pd.isna(data.Email_Id)) & pd.isna(data.FirstEmailDate), "FirstTransactionDate"]

# part of the day the transaction is complete. Devide day in 24 categories according to hour
data.insert(4, "TxnHour", list(map(date_hour, data["TxnCompleteTime"]))  )

#duration duration
data.insert(5, "TxnDuration_Seconds", list( map(date_diff, data["TxnInitTime"], data["TxnCompleteTime"]) )  )

data["TxnDuration_Seconds"] = data["TxnDuration_Seconds"].astype(int)

#weekday (0-6) in which the transaction has been made
data.insert(6, "TxnWeekday", list(map(date_weekday, data["TxnCompleteTime"])))

#days from the first transaction
data.insert(7, "FromFirstTxnToCurrentTxn_Days", list(map(date_diff_days, data["FirstTransactionDate"], data["TxnCompleteTime"])) )

#number of transactions from IP
data.insert(9, "ClientIP_occurrences", hash_to_occurrences(data, "ClientIP") )

#number of transactions of a user for a specific digital service 
data.insert(11, "User_Id_occurrences", hash_to_occurrences(data, "User_Id") )

#GeoIp info does not coincide with Alpha2Code info
data.insert(16, "GeoIpNotEqAlpha2", ( ~( (data.GeoIpCountry == data.Alpha2Code) | pd.isnull(data.GeoIpCountry) ) ).astype(int) )

#number of transactions done with a given email
data.insert(19, "Email_Id_occurrences", hash_to_occurrences(data, "Email_Id") )

#email is entered after the first transaction
data.insert(21, "FirstTransactionWithoutEmail",~(data.FirstEmailDate <= data.FirstTransactionDate))
#same as 
#data.insert(21, "FirstTransactionWithoutEmail", (data.FirstTransactionDate < data.FirstEmailDate) | (pd.isnull(data.FirstEmailDate) ))

data["FirstTransactionWithoutEmail"] = data["FirstTransactionWithoutEmail"].astype(int)

#replace NaN with 0
data.GeoIpCountry.fillna(0, inplace=True)

data.Email_Id_occurrences.fillna(0, inplace=True)

#Alpha2Code and GeoIpCountry with same codes
data = data.astype(
    {"Alpha2Code":"category", 
     "GeoIpCountry":"category"}) 


geoIpCountry_dic =  pd.Series(data.GeoIpCountry.cat.codes.values, index=data.GeoIpCountry).to_dict()

data["GeoIpCountry"] = data["GeoIpCountry"].map(geoIpCountry_dic)

data["Alpha2Code"] = data["Alpha2Code"].map(geoIpCountry_dic)

# set appropriate type
data = data.astype({
    "Alpha2Code":"int", 
    "GeoIpCountry":"int", 
    "Email_Id_occurrences":"int"}) 

#add an interaction term
data["TxnDuration_X_UniquePaymentCh"] = data.TxnDuration_Seconds * data.UniquePaymentChannel

# TODO 
# with a smaller training set (for efficiency), add all the polinomial features up to a degree
# poly = PolynomialFeatures(6)
# X = poly.fit_transform(X) 
# and select the features (from sklearn.feature_selection import RFE) that are relevant, 
# then train the entire training set with these relevant features

#delete unuseful features
data.drop(columns=[
    "Transaction_Id", 
    "TxnInitTime", 
    "TxnCompleteTime", 
    "ClientIP", 
    "User_Id", 
    "Email_Id", 
    "FirstTransactionDate", 
    "FirstEmailDate"], 
    inplace = True)

data.shape

data.info()

information(data)

#outliers with Tukey's method
q1 = data.TxnDuration_Seconds.quantile(0.25)
q3 = data.TxnDuration_Seconds.quantile(0.75)
iqr = q3 - q1
low_lim = q1 - 2 * iqr
up_lim = q3 + 2 * iqr

#percentage of TxnDuration_Seconds outliers with flag 1 over all the flag 1, as in 
100* (((data.TxnDuration_Seconds < low_lim) | (data.TxnDuration_Seconds > up_lim )) & data.Flag).sum() / (data.Flag == 1).sum()

#many outliers with flag 1  -->  keep outliers and use Winsorize method

sns.boxplot(x = data.User_Id_occurrences)
plt.show()

winsorize(data.User_Id_occurrences, (0.01,0.01), inplace=True)

sns.boxplot(x = data.FromFirstTxnToCurrentTxn_Days)
plt.show()

winsorize(data.FromFirstTxnToCurrentTxn_Days, (0.01,0.01), inplace=True)

sns.boxplot(x = data.UniquePaymentChannel)
plt.show()

winsorize(data.UniquePaymentChannel, (0.01,0.01), inplace=True)

sns.boxplot(x = data.ClientIP_occurrences)
plt.show()

winsorize(data.ClientIP_occurrences, (0.01,0.01), inplace=True)

sns.boxplot(x = data.TxnDuration_Seconds)
plt.show()

winsorize(data.TxnDuration_Seconds, (0.01,0.01), inplace=True)

sns.boxplot(x = data.Email_Id_occurrences)
plt.show()

winsorize(data.Email_Id_occurrences, (0.01,0.01), inplace=True)

sns.boxplot(x = data.TxnDuration_X_UniquePaymentCh)
plt.show()

winsorize(data.TxnDuration_X_UniquePaymentCh, (0.01,0.01), inplace=True)

#set X and Y
X = data.drop(columns=['Flag'])
Y = data.Flag

#standardization

scaler = preprocessing.StandardScaler().fit(X)

X = scaler.transform(X) 

#divide data in 60% train, 20% cross validation, 20% test
X_train, X_cv_t, Y_train, Y_cv_t = train_test_split(X, Y, test_size=0.4, random_state=0)
X_cross_val, X_test, Y_cross_val, Y_test = train_test_split(X_cv_t, Y_cv_t, test_size=0.5, random_state=0)
del X_cv_t, Y_cv_t

#fit logistic regression with lambda = 1
model = LogisticRegression(class_weight="balanced")
model.fit(X_train, Y_train)

#predict vaues on test set using threshold =0.5
Y_test_pred_not_optimised = model.predict(X_test)

#model performance
Y_test_pred_not_optimised_series = pd.Series(list(Y_test_pred_not_optimised))
Y_test_series = pd.Series(list(Y_test))

print("Model performance with not optimised hyperparameters")
print("----------------------------------------------------")
#percentage of true positive
tp_not_optimised = ((Y_test_pred_not_optimised_series == 1) & (Y_test_series == 1)).sum() 
print("percentage of true positive", 100*tp_not_optimised/len(Y_test_series))
#percentage of false positive
fp_not_optimised = ((Y_test_pred_not_optimised_series == 1) & (Y_test_series == 0)).sum()
print("percentage of false positive", 100*fp_not_optimised/len(Y_test_series))
#percentage of false negative
fn_not_optimised = ((Y_test_pred_not_optimised_series == 0) & (Y_test_series == 1)).sum() 
print("percentage of false negative", 100*fn_not_optimised/len(Y_test_series))
#percentage of true negative
tn_not_optimised = ((Y_test_pred_not_optimised_series == 0) & (Y_test_series == 0)).sum() 
print("percentage of true negative", 100*tn_not_optimised/len(Y_test_series))
print("----------------------------------------------------")
print("number of true positive", tp_not_optimised)
print("number of false positive", fp_not_optimised)
print("number of false negative", fn_not_optimised)
print("number of true negative", tn_not_optimised)
print("----------------------------------------------------")
#F1 score for the test set with not optimised hyperparameters
precision_not_optimised = tp_not_optimised/(tp_not_optimised+fp_not_optimised)
recall_not_optimised = tp_not_optimised/(tp_not_optimised+fn_not_optimised)
print("F1 score", 2*precision_not_optimised*recall_not_optimised/(precision_not_optimised+recall_not_optimised) )

#this cell takes about 2 minutes to compute
#maximise F1 score over C and thresholds hyperparameters
bnds = ((0, None), (0 , 1)) # ( (1/lambda bounds) , (threshold bounds) )
res = optimize.minimize(f1_score_neg, (1, 0.5), bounds=bnds, method='Powell')
print(res)

#regularisation parameter C = lambda^(-1)
lambda_inv = res.x[0]

#threshold
threshold = res.x[1]

#Measure the model performance with optimised hyperparameters
model = LogisticRegression(class_weight="balanced", C=lambda_inv)
model.fit(X_test, Y_test)
Y_test_pred = predicted_y_with_threshold(model, X_test,threshold)

Y_test_pred_series = pd.Series(list(Y_test_pred))

print("Model performance with optimised hyperparameters")
print("------------------------------------------------")
#percentage of true positive
tp = ((Y_test_pred_series == 1) & (Y_test_series == 1)).sum() 
print("percentage of true positive", 100*tp/len(Y_test_series))
#percentage of false positive
fp = ((Y_test_pred_series == 1) & (Y_test_series == 0)).sum()
print("percentage of false positive", 100*fp/len(Y_test_series))
#percentage of false negative
fn = ((Y_test_pred_series == 0) & (Y_test_series == 1)).sum() 
print("percentage of false negative", 100*fn/len(Y_test_series))
#percentage of true negative
tn = ((Y_test_pred_series == 0) & (Y_test_series == 0)).sum() 
print("percentage of true negative", 100*tn/len(Y_test_series))
print("------------------------------------------------")
print("number of true positive", tp)
print("number of false positive", fp)
print("number of false negative", fn)
print("number of true negative", tn)
print("------------------------------------------------")
#F1 score for the test set with optimised hyperparameters
print("F1 score", f1_score(Y_test, Y_test_pred))

# just to check
precision = tp/(tp+fp)
recall = tp/(tp+fn)
2*precision*recall/(precision+recall) == f1_score(Y_test, Y_test_pred)

#this cell takes about 4 minutes to compute
#maximise the _modified_ F1 score over C and thresholds hyperparameters
Y_cross_val_series = pd.Series(list(Y_cross_val))
bnds1 = ((0, None), (0 , 1)) # ( (1/lambda bounds) , (threshold bounds) )
res1 = optimize.minimize(f1_score_neg_mod_th, (1, 0.5), bounds=bnds1, method='Powell')
print(res1)

#regularisation parameter C = lambda^(-1) with new optimisation
lambda_inv1 = res1.x[0]

#threshold with new optimisation
threshold1 = res1.x[1]

#test the model with new optimised hyperparameters
model = LogisticRegression(class_weight="balanced", C=lambda_inv1)
model.fit(X_test, Y_test)
Y_test_pred1 = predicted_y_with_threshold(model, X_test, threshold1)

Y_test_pred_series1 = pd.Series(list(Y_test_pred1))

print("Model performance with new optimised hyperparameters")
print("----------------------------------------------------")
#percentage of true positive
tp = ((Y_test_pred_series1 == 1) & (Y_test_series == 1)).sum() 
print("percentage of true positive", 100*tp/len(Y_test_series))
#percentage of false positive
fp = ((Y_test_pred_series1 == 1) & (Y_test_series == 0)).sum()
print("percentage of false positive", 100*fp/len(Y_test_series))
#percentage of false negative
fn = ((Y_test_pred_series1 == 0) & (Y_test_series == 1)).sum() 
print("percentage of false negative", 100*fn/len(Y_test_series))
#percentage of true negative
tn = ((Y_test_pred_series1 == 0) & (Y_test_series == 0)).sum() 
print("percentage of true negative", 100*tn/len(Y_test_series))
print("----------------------------------------------------")
print("number of true positive", tp)
print("number of false positive", fp)
print("number of false negative", fn)
print("number of true negative", tn)
print("----------------------------------------------------")
#F1 score for the test set
print("F1 score", f1_score(Y_test, Y_test_pred1))



















# TODO add new features
# calculate the number of occurrences of 
# Merchant_Id
# Price
# ChannelType
# Alpha2Code
# GeoIpCountry
# GeoIpNotEqAlpha2
# UserAgent
# FirstTransactionWithoutEmail
# Flag
# PaymentChannel
# ItemName
# eg
# data.insert(2, "Merchant_Id_occurrences", hash_to_occurrences(data, "Merchant_Id") )
# 
# TODO for all features, except the ones above, add a column to report the outliers (1 if el is outliers 0 if not) eg
# 
# data_copy = data.copy()
# added_cols_num = 0
# 
# for idx, col in enumerate(data_copy.columns):
#     min = data[col].quantile(0.01)
#     max = data[col].quantile(0.99)
#     
#     if ((data[col] < min) | (data[col] > max )).any():
#         header = "_".join([col,"outlier"])
#         vals = ( (data[col] < min) | (data[col] > max ) ).astype(int)
#         data.insert(idx+added_cols_num+1, header, vals )
#         added_cols_num += 1
#         
#         print(header)
#         
# del data_copy
# print(added_cols_num, "feature(s) added.")
# 
# 
# 
#         
# for each user calculate the historical mean of the prices. For each entry calculate the difference
# between the current price and the mean, over the std dev
# 
# dic_UserId_mean_price = (data.groupby("User_Id").mean().Price).to_dict()
# 
# data["Test_mean_price"] = data["User_Id"].map(dic_UserId_mean_price, na_action="ignore")
# 
# dic_UserId_std_dev = (data.groupby("User_Id").std().Price).to_dict()
# 
# data["Test_std_dev_price"] = data["User_Id"].map(dic_UserId_std_dev, na_action="ignore")
# 
# data["Test_dist_from_mean"] = (data.Price-data.Test_mean_price)/data.Test_std_dev_price
